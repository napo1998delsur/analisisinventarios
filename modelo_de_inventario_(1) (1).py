# -*- coding: utf-8 -*-
"""modelo-de-inventario (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O2kfqhZPOtDxJOUHku2ayxYfalD-Rumk

Gradient Boosting


Gradient Boosting es una generalización del algoritmo AdaBoost que permite emplear cualquier función de coste, siempre que esta sea diferenciable. La flexibilidad de este algoritmo ha hecho posible aplicar boosting a multitud de problemas (regresión, clasificación múltiple...) convirtiéndolo en uno de los métodos de machine learning de mayor éxito. Si bien existen varias adaptaciones, la idea general de todas ellas es la misma: entrenar modelos de forma secuencial, de forma que cada modelo ajusta los residuos (errores) de los modelos anteriores.

Se ajusta un primer weak learner  f1  con el que se predice la variable respuesta  y , y se calculan los residuos  y−f1(x) . A continuación, se ajusta un nuevo modelo  f2 , que intenta predecir los residuos del modelo anterior, en otras palabras, trata de corregir los errores que ha hecho el modelo  f1 .

f1(x)≈y
 
f2(x)≈y−f1(x)
 
En la siguiente iteración, se calculan los residuos de los dos modelos de forma conjunta  y−f1(x)−f2(x) , los errores cometidos por  f1  y que  f2  no ha sido capaz de corregir, y se ajusta un tercer modelo  f3  para tratar de corregirlos.

f3(x)≈y−f1(x)−f2(x)
 
Este proceso se repite  M  veces, de forma que cada nuevo modelo minimiza los residuos (errores) del anterior.

Dado que el objetivo de Gradient Boosting es ir minimizando los residuos iteración a iteración, es susceptible de overfitting. Una forma de evitar este problema es empleando un valor de regularización, también conocido como learning rate ( λ ), que limite la influencia de cada modelo en el conjunto del ensemble. Como consecuencia de esta regularización, se necesitan más modelos para formar el ensemble pero se consiguen mejores resultados.

f1(x)≈y
 
f2(x)≈y−λf1(x)
 
f3(x)≈y−λf1(x)−λf2(x)
 
y≈λf1(x)+λf2(x)+λf3(x)+ ... +λfm(x)

MODELO DE PREDICCION DE CANTIDAD ECONOMICA DE PEDIDO

DELSUR
"""

# librarie import
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns

# Import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ParameterGrid
from sklearn.inspection import permutation_importance
import multiprocessing
# Import Linear Regression
from sklearn.linear_model import LinearRegression


# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

datos=pd.read_csv('/content/final-data 2018-2022 (1).csv')

datos.head()

datos.columns

datos.isnull().sum()

"""GRAFICOS DE VARIABLES CLAVES"""

clean=datos.dropna(subset=['Fecha Financ.','Nombre artículo','Tipo de Movimiento'])

px.histogram(clean, x='Fecha Financ.', color='Grupo artículo', barmode='group')

datos.corr()

# buscar valores nulos
datos.isnull().sum()

datos.shape

datos.info()

print(datos['Almacén'].unique())

# modelando variables categoricas a numericas
almacen  = {'PC01':7 ,
                'PC12': 6,
                'PC03': 5,
                'PC04': 4,
                'PC09': 3,
                'PC14': 2,
                'PC15':1,
                'PC16':0
                }

datos['Prioridad_map'] = datos['Almacén'].map(almacen)

print(datos['Prioridad_map'].unique())

print(datos['Tipo Art.'].unique())

print(datos['Tipo de Movimiento'].unique())

movimiento  = {'Consumo':4 ,
                'Devolución': 3,
                'Ingreso': 2,
                'Traslado': 1
                }

datos['movimiento_map'] = datos['Tipo de Movimiento'].map(movimiento)

print(datos['movimiento_map'].unique())

"""ANALISIS DE PEARSON DE CORRELACCION ESTADISTICA"""

# MATRIZ DE CORRELLACION DE VARIABLES


# encontrar correlAacciones matematicas
def tidy_corr_matrix(corr_mat):
    '''
    Función para convertir una matriz de correlación de pandas en formato tidy
    '''
    corr_mat = corr_mat.stack().reset_index()
    corr_mat.columns = ['variable_1','variable_2','r']
    corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]
    corr_mat['abs_r'] = np.abs(corr_mat['r'])
    corr_mat = corr_mat.sort_values('abs_r', ascending=False)
    
    return(corr_mat)
corr_matrix = datos.select_dtypes(include=['float64', 'int']).corr(method='pearson')
tidy_corr_matrix(corr_matrix).head(20)

# Heatmap matriz de correlaciones
# ==============================================================================
fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,20
                                                  ))

sns.heatmap(
    corr_matrix,
    annot     = True,
    cbar      = False,
    annot_kws = {"size": 8},
    vmin      = -1,
    vmax      = 1,
    center    = 0,
    cmap      = sns.diverging_palette(20, 220, n=200),
    square    = True,
    ax        = ax
)

ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation = 45,
    horizontalalignment = 'right',
)

ax.tick_params(labelsize = 10)

datos.columns

datos.info()

datos['Fecha Financ.'].iloc[:5]

datos.columns

# define linear regression


class LinearRegressionGD:
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])
        self.b_ = np.array([0.])
        self.losses_ = []

        for i in range(self.n_iter):
            output = self.net_input(X)
            errors = (y - output)
            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            self.b_ += self.eta * 2.0 * errors.mean()
            loss = (errors**2).mean()
            self.losses_.append(loss)
        return self

    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_

    def predict(self, X):
        return self.net_input(X)

datos['Prioridad_map']=pd.to_numeric(datos['Prioridad_map'])
datos['movimiento_map']=pd.to_numeric(datos['movimiento_map'])

datos['Cód. artículo']=pd.to_numeric(datos['Cód. artículo'],errors='coerce')

datos.isnull().sum()

clean=datos.dropna(subset=['Cód. artículo','Prioridad_map','movimiento_map','MES','NAT','Cantidad',])

clean.corr()

clean['Cód. artículo'].nunique()

clean.info()

df=clean.to_csv('clean.csv')

"""MODELADO"""

inventario=pd.read_csv('/content/clean.csv')

inventario.info()

inventario['Nombre artículo'].nunique()

inventario.columns

inventario.dtypes

# datos

from sklearn import linear_model
regr = linear_model.LinearRegression()
X = inventario[['AÑO','Cód. artículo','Prioridad_map','movimiento_map','MES','NAT','month','day','Cantidad ABS' ]]
y = inventario[['Cantidad']]
# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=123)


# Import XGBRegressor
from xgboost import XGBRegressor
# Import mean_squared_error
from sklearn.metrics import mean_squared_error

# Instantiate the XGBRegressor, xg_reg
xg_reg = XGBRegressor()

# Fit xg_reg to training set
xg_reg.fit(X_train, y_train)

# Predict labels of test set, y_pred
y_pred = xg_reg.predict(X_test)

# Compute the mean_squared_error, mse
mse = mean_squared_error(y_test, y_pred)

# Compute the root mean squared error, rmse
rmse = np.sqrt(mse)

# Display the root mean squared error
print("RMSE: %0.2f" % (rmse))

# Import cross_val_score
from sklearn.model_selection import cross_val_score

# Instantiate Linear Regression
model = LinearRegression()

# Obtain scores of cross-validation using 10 splits and mean squared error
scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)

# Take square root of the scores
rmse = np.sqrt(-scores)

# Display root mean squared error
print('Reg rmse:', np.round(rmse, 2))

# Display mean score
print('RMSE mean: %0.2f' % (rmse.mean()))

# Instantiate XGBRegressor
model = XGBRegressor(objective="reg:squarederror")

# Obtain scores of cross-validation using 10 splits and mean squared error
scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)

# Take square root of the scores
rmse = np.sqrt(-scores)

# Display root mean squared error
print('Reg rmse:', np.round(rmse, 2))

# Display mean score
print('RMSE mean: %0.2f' % (rmse.mean()))

y_train_pred =xg_reg.predict(X_train)
y_test_pred = xg_reg.predict(X_test)
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
mae_train = mean_absolute_error(y_train, y_train_pred)
mae_test = mean_absolute_error(y_test, y_test_pred)
print(f'MAE train: {mae_train:.2f}')
print(f'MAE test: {mae_test:.2f}')


r2_train = r2_score(y_train, y_train_pred)
r2_test =r2_score(y_test, y_test_pred)
print(f'R^2 train: {r2_train:.2f}')
print(f'R^2 test: {r2_test:.2f}')

test=X_test.to_csv('test.csv')

